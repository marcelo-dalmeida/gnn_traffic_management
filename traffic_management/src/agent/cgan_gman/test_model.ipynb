{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.keras.layers.Input((32, 10, 12))\n",
    "X.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'keras.src.engine.keras_tensor.KerasTensor'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dkuts\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\dkuts\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1102\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1108\u001b[0m         )\n\u001b[0;32m   1109\u001b[0m     )\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1115\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'keras.src.engine.keras_tensor.KerasTensor'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "N = X.shape[3]\n",
    "Q = X.shape[2]\n",
    "input_shape = X.shape[1:]  # Shape without batch size\n",
    "X_train_flat = tf.reshape(X, (-1, Q * N))\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a fully connected layer with sigmoid activation\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(64, activation='relu'))  # You can adjust the number of units\n",
    "model.add(Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv2d(x, output_dims, kernel_size, stride=[1, 1],\n",
    "           padding='SAME', use_bias=True, activation=tf.nn.relu,\n",
    "           bn=False, bn_decay=None, is_training=None):\n",
    "    input_dims = x.get_shape()[-1].value\n",
    "    kernel_shape = kernel_size + [input_dims, output_dims]\n",
    "    kernel = tf.Variable(\n",
    "        tf.glorot_uniform_initializer()(shape=kernel_shape),\n",
    "        dtype=tf.float32, trainable=True, name='kernel')\n",
    "    x = tf.nn.conv2d(x, kernel, [1] + stride + [1], padding=padding)\n",
    "    if use_bias:\n",
    "        bias = tf.Variable(\n",
    "            tf.zeros_initializer()(shape=[output_dims]),\n",
    "            dtype=tf.float32, trainable=True, name='bias')\n",
    "        x = tf.nn.bias_add(x, bias)\n",
    "    if activation is not None:\n",
    "        if bn:\n",
    "            x = batch_norm(x, is_training=is_training, bn_decay=bn_decay)\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def batch_norm(x, is_training, bn_decay):\n",
    "    input_dims = x.get_shape()[-1].value\n",
    "    moment_dims = list(range(len(x.get_shape()) - 1))\n",
    "    beta = tf.Variable(\n",
    "        tf.zeros_initializer()(shape=[input_dims]),\n",
    "        dtype=tf.float32, trainable=True, name='beta')\n",
    "    gamma = tf.Variable(\n",
    "        tf.ones_initializer()(shape=[input_dims]),\n",
    "        dtype=tf.float32, trainable=True, name='gamma')\n",
    "    batch_mean, batch_var = tf.nn.moments(x, moment_dims, name='moments')\n",
    "\n",
    "    decay = bn_decay if bn_decay is not None else 0.9\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
    "    # Operator that maintains moving averages of variables.\n",
    "    ema_apply_op = tf.cond(\n",
    "        is_training,\n",
    "        lambda: ema.apply([batch_mean, batch_var]),\n",
    "        lambda: tf.no_op())\n",
    "\n",
    "    # Update moving average and return current batch's avg and var.\n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "    # ema.average returns the Variable holding the average of var.\n",
    "    mean, var = tf.cond(\n",
    "        is_training,\n",
    "        mean_var_with_update,\n",
    "        lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "    x = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return x\n",
    "\n",
    "\n",
    "def dropout(x, drop, is_training):\n",
    "    x = tf.cond(\n",
    "        is_training,\n",
    "        lambda: tf.nn.dropout(x, rate=drop),\n",
    "        lambda: x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def placeholder(P, Q, N):\n",
    "    X = tf.compat.v1.placeholder(\n",
    "        shape=(None, P, N), dtype=tf.float32, name='X')\n",
    "    TE = tf.compat.v1.placeholder(\n",
    "        shape=(None, P + Q, 2), dtype=tf.int32, name='TE')\n",
    "    label = tf.compat.v1.placeholder(\n",
    "        shape=(None, Q, N), dtype=tf.float32, name='label')\n",
    "    is_training = tf.compat.v1.placeholder(\n",
    "        shape=(), dtype=tf.bool, name='is_training')\n",
    "    return X, TE, label, is_training\n",
    "\n",
    "\n",
    "def FC(x, units, activations, bn, bn_decay, is_training, use_bias=True, drop=None):\n",
    "    if isinstance(units, int):\n",
    "        units = [units]\n",
    "        activations = [activations]\n",
    "    elif isinstance(units, tuple):\n",
    "        units = list(units)\n",
    "        activations = list(activations)\n",
    "    assert type(units) == list\n",
    "    for num_unit, activation in zip(units, activations):\n",
    "        if drop is not None:\n",
    "            x = dropout(x, drop=drop, is_training=is_training)\n",
    "        x = conv2d(\n",
    "            x, output_dims=num_unit, kernel_size=[1, 1], stride=[1, 1],\n",
    "            padding='VALID', use_bias=use_bias, activation=activation,\n",
    "            bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    return x\n",
    "\n",
    "\n",
    "def STEmbedding(SE, TE, T, D, bn, bn_decay, is_training):\n",
    "    '''\n",
    "    spatio-temporal embedding\n",
    "    SE:     [N, D]\n",
    "    TE:     [batch_size, P + Q, 2] (dayofweek, timeofday)\n",
    "    T:      num of time steps in one day\n",
    "    D:      output dims\n",
    "    retrun: [batch_size, P + Q, N, D]\n",
    "    '''\n",
    "    # spatial embedding\n",
    "    SE = tf.expand_dims(tf.expand_dims(SE, axis=0), axis=0)\n",
    "    SE = FC(\n",
    "        SE, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    # temporal embedding\n",
    "    dayofweek = tf.one_hot(TE[..., 0], depth=7)\n",
    "    timeofday = tf.one_hot(TE[..., 1], depth=T)\n",
    "    TE = tf.concat((dayofweek, timeofday), axis=-1)\n",
    "    TE = tf.expand_dims(TE, axis=2)\n",
    "    TE = FC(\n",
    "        TE, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    return tf.add(SE, TE)\n",
    "\n",
    "\n",
    "def spatialAttention(X, STE, K, d, bn, bn_decay, is_training):\n",
    "    '''\n",
    "    spatial attention mechanism\n",
    "    X:      [batch_size, num_step, N, D]\n",
    "    STE:    [batch_size, num_step, N, D]\n",
    "    K:      number of attention heads\n",
    "    d:      dimension of each attention outputs\n",
    "    return: [batch_size, num_step, N, D]\n",
    "    '''\n",
    "    D = K * d\n",
    "    X = tf.concat((X, STE), axis=-1)\n",
    "    # [batch_size, num_step, N, K * d]\n",
    "    query = FC(\n",
    "        X, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    key = FC(\n",
    "        X, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    value = FC(\n",
    "        X, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    # [K * batch_size, num_step, N, d]\n",
    "    query = tf.concat(tf.split(query, K, axis=-1), axis=0)\n",
    "    key = tf.concat(tf.split(key, K, axis=-1), axis=0)\n",
    "    value = tf.concat(tf.split(value, K, axis=-1), axis=0)\n",
    "    # [K * batch_size, num_step, N, N]\n",
    "    attention = tf.matmul(query, key, transpose_b=True)\n",
    "    attention /= (d ** 0.5)\n",
    "    attention = tf.nn.softmax(attention, axis=-1)\n",
    "    # [batch_size, num_step, N, D]\n",
    "    X = tf.matmul(attention, value)\n",
    "    X = tf.concat(tf.split(X, K, axis=0), axis=-1)\n",
    "    X = FC(\n",
    "        X, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    return X\n",
    "\n",
    "\n",
    "def temporalAttention(X, STE, K, d, bn, bn_decay, is_training, mask=True):\n",
    "    '''\n",
    "    temporal attention mechanism\n",
    "    X:      [batch_size, num_step, N, D]\n",
    "    STE:    [batch_size, num_step, N, D]\n",
    "    K:      number of attention heads\n",
    "    d:      dimension of each attention outputs\n",
    "    return: [batch_size, num_step, N, D]\n",
    "    '''\n",
    "    D = K * d\n",
    "    X = tf.concat((X, STE), axis=-1)\n",
    "    # [batch_size, num_step, N, K * d]\n",
    "    query = FC(\n",
    "        X, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    key = FC(\n",
    "        X, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    value = FC(\n",
    "        X, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    # [K * batch_size, num_step, N, d]\n",
    "    query = tf.concat(tf.split(query, K, axis=-1), axis=0)\n",
    "    key = tf.concat(tf.split(key, K, axis=-1), axis=0)\n",
    "    value = tf.concat(tf.split(value, K, axis=-1), axis=0)\n",
    "    # query: [K * batch_size, N, num_step, d]\n",
    "    # key:   [K * batch_size, N, d, num_step]\n",
    "    # value: [K * batch_size, N, num_step, d]\n",
    "    query = tf.transpose(query, perm=(0, 2, 1, 3))\n",
    "    key = tf.transpose(key, perm=(0, 2, 3, 1))\n",
    "    value = tf.transpose(value, perm=(0, 2, 1, 3))\n",
    "    # [K * batch_size, N, num_step, num_step]\n",
    "    attention = tf.matmul(query, key)\n",
    "    attention /= (d ** 0.5)\n",
    "    # mask attention score\n",
    "    if mask:\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        num_step = X.get_shape()[1].value\n",
    "        N = X.get_shape()[2].value\n",
    "        mask = tf.ones(shape=(num_step, num_step))\n",
    "        mask = tf.linalg.LinearOperatorLowerTriangular(mask).to_dense()\n",
    "        mask = tf.expand_dims(tf.expand_dims(mask, axis=0), axis=0)\n",
    "        mask = tf.tile(mask, multiples=(K * batch_size, N, 1, 1))\n",
    "        mask = tf.cast(mask, dtype=tf.bool)\n",
    "        attention = tf.compat.v2.where(\n",
    "            condition=mask, x=attention, y=-2 ** 15 + 1)\n",
    "    # softmax\n",
    "    attention = tf.nn.softmax(attention, axis=-1)\n",
    "    # [batch_size, num_step, N, D]\n",
    "    X = tf.matmul(attention, value)\n",
    "    X = tf.transpose(X, perm=(0, 2, 1, 3))\n",
    "    X = tf.concat(tf.split(X, K, axis=0), axis=-1)\n",
    "    X = FC(\n",
    "        X, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    return X\n",
    "\n",
    "\n",
    "def gatedFusion(HS, HT, D, bn, bn_decay, is_training):\n",
    "    '''\n",
    "    gated fusion\n",
    "    HS:     [batch_size, num_step, N, D]\n",
    "    HT:     [batch_size, num_step, N, D]\n",
    "    D:      output dims\n",
    "    return: [batch_size, num_step, N, D]\n",
    "    '''\n",
    "    XS = FC(\n",
    "        HS, units=D, activations=None,\n",
    "        bn=bn, bn_decay=bn_decay,\n",
    "        is_training=is_training, use_bias=False)\n",
    "    XT = FC(\n",
    "        HT, units=D, activations=None,\n",
    "        bn=bn, bn_decay=bn_decay,\n",
    "        is_training=is_training, use_bias=True)\n",
    "    z = tf.nn.sigmoid(tf.add(XS, XT))\n",
    "    H = tf.add(tf.multiply(z, HS), tf.multiply(1 - z, HT))\n",
    "    H = FC(\n",
    "        H, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    return H\n",
    "\n",
    "\n",
    "def STAttBlock(X, STE, K, d, bn, bn_decay, is_training, mask=True):\n",
    "    HS = spatialAttention(X, STE, K, d, bn, bn_decay, is_training)\n",
    "    HT = temporalAttention(X, STE, K, d, bn, bn_decay, is_training, mask=mask)\n",
    "    H = gatedFusion(HS, HT, K * d, bn, bn_decay, is_training)\n",
    "    return tf.add(X, H)\n",
    "\n",
    "\n",
    "def transformAttention(X, STE_P, STE_Q, K, d, bn, bn_decay, is_training):\n",
    "    '''\n",
    "    transform attention mechanism\n",
    "    X:      [batch_size, P, N, D]\n",
    "    STE_P:  [batch_size, P, N, D]\n",
    "    STE_Q:  [batch_size, Q, N, D]\n",
    "    K:      number of attention heads\n",
    "    d:      dimension of each attention outputs\n",
    "    return: [batch_size, Q, N, D]\n",
    "    '''\n",
    "    D = K * d\n",
    "    # query: [batch_size, Q, N, K * d]\n",
    "    # key:   [batch_size, P, N, K * d]\n",
    "    # value: [batch_size, P, N, K * d]\n",
    "    query = FC(\n",
    "        STE_Q, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    key = FC(\n",
    "        STE_P, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    value = FC(\n",
    "        X, units=D, activations=tf.nn.relu,\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    # query: [K * batch_size, Q, N, d]\n",
    "    # key:   [K * batch_size, P, N, d]\n",
    "    # value: [K * batch_size, P, N, d]\n",
    "    query = tf.concat(tf.split(query, K, axis=-1), axis=0)\n",
    "    key = tf.concat(tf.split(key, K, axis=-1), axis=0)\n",
    "    value = tf.concat(tf.split(value, K, axis=-1), axis=0)\n",
    "    # query: [K * batch_size, N, Q, d]\n",
    "    # key:   [K * batch_size, N, d, P]\n",
    "    # value: [K * batch_size, N, P, d]\n",
    "    query = tf.transpose(query, perm=(0, 2, 1, 3))\n",
    "    key = tf.transpose(key, perm=(0, 2, 3, 1))\n",
    "    value = tf.transpose(value, perm=(0, 2, 1, 3))\n",
    "    # [K * batch_size, N, Q, P]\n",
    "    attention = tf.matmul(query, key)\n",
    "    attention /= (d ** 0.5)\n",
    "    attention = tf.nn.softmax(attention, axis=-1)\n",
    "    # [batch_size, Q, N, D]\n",
    "    X = tf.matmul(attention, value)\n",
    "    X = tf.transpose(X, perm=(0, 2, 1, 3))\n",
    "    X = tf.concat(tf.split(X, K, axis=0), axis=-1)\n",
    "    X = FC(\n",
    "        X, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    return X\n",
    "\n",
    "\n",
    "def GMAN(X, TE, SE,  T, bn, bn_decay, is_training):\n",
    "    '''\n",
    "    GMAN\n",
    "    X:       [batch_size, P, N]\n",
    "    TE:      [batch_size, P + Q, 2] (time-of-day, day-of-week)\n",
    "    SE:      [N, K * d]\n",
    "    P:       number of history steps\n",
    "    Q:       number of prediction steps\n",
    "    T:       one day is divided into T steps\n",
    "    L:       number of STAtt blocks in the encoder/decoder\n",
    "    K:       number of attention heads\n",
    "    d:       dimension of each attention head outputs\n",
    "    return:  [batch_size, Q, N]\n",
    "    '''\n",
    "\n",
    "    P = config.AGENT.HISTORY_STEPS\n",
    "    Q = config.AGENT.PREDICTION_STEPS\n",
    "    L = config.AGENT.NUMBER_OF_STATT_BLOCKS\n",
    "    K = config.AGENT.NUMBER_OF_ATTENTION_HEADS\n",
    "    d = config.AGENT.HEAD_ATTENTION_OUTPUT_DIM\n",
    "\n",
    "    D = K * d\n",
    "    # input\n",
    "    X = tf.expand_dims(X, axis=-1)\n",
    "\n",
    "    X = FC(\n",
    "        X, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    # STE\n",
    "    STE = STEmbedding(SE, TE, T, D, bn, bn_decay, is_training)\n",
    "    STE_P = STE[:, : P]\n",
    "    STE_Q = STE[:, P:]\n",
    "    # encoder\n",
    "    for _ in range(L):\n",
    "        X = STAttBlock(X, STE_P, K, d, bn, bn_decay, is_training)\n",
    "    # transAtt\n",
    "    X = transformAttention(\n",
    "        X, STE_P, STE_Q, K, d, bn, bn_decay, is_training)\n",
    "    # decoder\n",
    "    for _ in range(L):\n",
    "        X = STAttBlock(X, STE_Q, K, d, bn, bn_decay, is_training)\n",
    "    # output\n",
    "    X = FC(\n",
    "        X, units=[D, 1], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training,\n",
    "        use_bias=True, drop=0.1)\n",
    "    return tf.squeeze(X, axis=3)\n",
    "\n",
    "\n",
    "def GMAN_gen(X, TE, SE, Y, T, bn, bn_decay, is_training):\n",
    "    '''\n",
    "    GMAN\n",
    "    X:       [batch_size, P, N]\n",
    "    TE:      [batch_size, P + Q, 2] (time-of-day, day-of-week)\n",
    "    SE:      [N, K * d]\n",
    "    Y:       labels/conditions [batch_size, C] c = # of classes (3)\n",
    "    P:       number of history steps\n",
    "    Q:       number of prediction steps\n",
    "    T:       one day is divided into T steps\n",
    "    L:       number of STAtt blocks in the encoder/decoder\n",
    "    K:       number of attention heads\n",
    "    d:       dimension of each attention head outputs\n",
    "    return:  [batch_size, Q, N]\n",
    "    '''\n",
    "\n",
    "    P = config.AGENT.HISTORY_STEPS\n",
    "    Q = config.AGENT.PREDICTION_STEPS\n",
    "    L = config.AGENT.NUMBER_OF_STATT_BLOCKS\n",
    "    K = config.AGENT.NUMBER_OF_ATTENTION_HEADS\n",
    "    d = config.AGENT.HEAD_ATTENTION_OUTPUT_DIM\n",
    "\n",
    "    D = K * d\n",
    "    # input\n",
    "    X = tf.expand_dims(X, axis=-1)\n",
    "\n",
    "    X = tf.concat(X, Y)  # concat the label\n",
    "\n",
    "    X = FC(\n",
    "        X, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    # STE\n",
    "    STE = STEmbedding(SE, TE, T, D, bn, bn_decay, is_training)\n",
    "    STE_P = STE[:, : P]\n",
    "    STE_Q = STE[:, P:]\n",
    "    # encoder\n",
    "    for _ in range(L):\n",
    "        X = STAttBlock(X, STE_P, K, d, bn, bn_decay, is_training)\n",
    "    # transAtt\n",
    "    X = transformAttention(\n",
    "        X, STE_P, STE_Q, K, d, bn, bn_decay, is_training)\n",
    "    # decoder\n",
    "    for _ in range(L):\n",
    "        X = STAttBlock(X, STE_Q, K, d, bn, bn_decay, is_training)\n",
    "    # output\n",
    "    X = FC(\n",
    "        X, units=[D, 1], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training,\n",
    "        use_bias=True, drop=0.1)\n",
    "    return tf.squeeze(X, axis=3)\n",
    "\n",
    "\n",
    "def GMAN_disc(X, TE, SE, Y, T, bn, bn_decay, is_training):\n",
    "    '''\n",
    "    GMAN\n",
    "    X:       [batch_size, P, N]\n",
    "    TE:      [batch_size, P + Q, 2] (time-of-day, day-of-week)\n",
    "    SE:      [N, K * d]\n",
    "    Y:       labels/conditions [batch_size, C] c = # of classes (3)\n",
    "    P:       number of history steps\n",
    "    Q:       number of prediction steps\n",
    "    T:       one day is divided into T steps\n",
    "    L:       number of STAtt blocks in the encoder/decoder\n",
    "    K:       number of attention heads\n",
    "    d:       dimension of each attention head outputs\n",
    "    return:  [batch_size, Q, N]\n",
    "    '''\n",
    "\n",
    "    P = config.AGENT.HISTORY_STEPS\n",
    "    Q = config.AGENT.PREDICTION_STEPS\n",
    "    L = config.AGENT.NUMBER_OF_STATT_BLOCKS\n",
    "    K = config.AGENT.NUMBER_OF_ATTENTION_HEADS\n",
    "    d = config.AGENT.HEAD_ATTENTION_OUTPUT_DIM\n",
    "\n",
    "    D = K * d\n",
    "    # input\n",
    "    X = tf.expand_dims(X, axis=-1)\n",
    "\n",
    "    X = tf.concat(X, Y)  # concat the label\n",
    "\n",
    "    X = FC(\n",
    "        X, units=[D, D], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training)\n",
    "    # STE\n",
    "    STE = STEmbedding(SE, TE, T, D, bn, bn_decay, is_training)\n",
    "    STE_P = STE[:, : P]\n",
    "    STE_Q = STE[:, P:]\n",
    "    # encoder\n",
    "    for _ in range(L):\n",
    "        X = STAttBlock(X, STE_P, K, d, bn, bn_decay, is_training)\n",
    "    # transAtt\n",
    "    X = transformAttention(\n",
    "        X, STE_P, STE_Q, K, d, bn, bn_decay, is_training)\n",
    "    # decoder\n",
    "    for _ in range(L):\n",
    "        X = STAttBlock(X, STE_Q, K, d, bn, bn_decay, is_training)\n",
    "    # output\n",
    "    X = FC(\n",
    "        X, units=[D, 1], activations=[tf.nn.relu, None],\n",
    "        bn=bn, bn_decay=bn_decay, is_training=is_training,\n",
    "        use_bias=True, drop=0.1)\n",
    "    return tf.squeeze(X, axis=3)\n",
    "\n",
    "\n",
    "def mae_loss(pred, label):\n",
    "    mask = tf.not_equal(label, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    mask = tf.compat.v2.where(\n",
    "        condition=tf.math.is_nan(mask), x=0., y=mask)\n",
    "    loss = tf.abs(tf.subtract(pred, label))\n",
    "    loss *= mask\n",
    "    loss = tf.compat.v2.where(\n",
    "        condition=tf.math.is_nan(loss), x=0., y=loss)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# log string\n",
    "def log_string(log, string):\n",
    "    log.write(string + '\\n')\n",
    "    log.flush()\n",
    "    print(string)\n",
    "\n",
    "\n",
    "# metric\n",
    "def metric(pred, label):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mask = np.not_equal(label, 0)\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask /= np.mean(mask)\n",
    "        mae = np.abs(np.subtract(pred, label)).astype(np.float32)\n",
    "        rmse = np.square(mae)\n",
    "        mape = np.divide(mae, label)\n",
    "        mae = np.nan_to_num(mae * mask)\n",
    "        mae = np.mean(mae)\n",
    "        rmse = np.nan_to_num(rmse * mask)\n",
    "        rmse = np.sqrt(np.mean(rmse))\n",
    "        mape = np.nan_to_num(mape * mask)\n",
    "        mape = np.mean(mape)\n",
    "    return mae, rmse, mape\n",
    "\n",
    "\n",
    "def seq2instance(data, P, Q):\n",
    "    num_step, dims = data.shape\n",
    "    num_sample = num_step - P - Q + 1\n",
    "    x = np.zeros(shape=(num_sample, P, dims))\n",
    "    y = np.zeros(shape=(num_sample, Q, dims))\n",
    "    for i in range(num_sample):\n",
    "        x[i] = data[i: i + P]\n",
    "        y[i] = data[i + P: i + P + Q]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def loadData(dataset_file, attribute):\n",
    "    # Traffic\n",
    "    df = pd.read_hdf(os.path.join(config.ROOT_DIR, dataset_file),\n",
    "                     key='data').loc[:, attribute]\n",
    "    Traffic = df.values\n",
    "    # train/val/test\n",
    "    num_step = df.shape[0]\n",
    "    train_steps = round(config.AGENT.TRAIN_RATIO * num_step)\n",
    "    test_steps = round(config.AGENT.TEST_RATIO * num_step)\n",
    "    val_steps = num_step - train_steps - test_steps\n",
    "    train = Traffic[: train_steps]\n",
    "    val = Traffic[train_steps: train_steps + val_steps]\n",
    "    test = Traffic[-test_steps:]\n",
    "    # X, Y\n",
    "    trainX, trainY = seq2instance(\n",
    "        train, config.AGENT.HISTORY_STEPS, config.AGENT.PREDICTION_STEPS)\n",
    "    valX, valY = seq2instance(\n",
    "        val, config.AGENT.HISTORY_STEPS, config.AGENT.PREDICTION_STEPS)\n",
    "    testX, testY = seq2instance(\n",
    "        test, config.AGENT.HISTORY_STEPS, config.AGENT.PREDICTION_STEPS)\n",
    "    # normalization\n",
    "    mean, std = np.mean(trainX), np.std(trainX)\n",
    "    trainX = (trainX - mean) / std\n",
    "    valX = (valX - mean) / std\n",
    "    testX = (testX - mean) / std\n",
    "\n",
    "    se_file = os.path.join(config.ROOT_DIR, config.PATH_TO_RECORDS, 'SE.txt')\n",
    "\n",
    "    # spatial embedding\n",
    "    f = open(se_file, mode='r')\n",
    "    lines = f.readlines()\n",
    "    temp = lines[0].split(' ')\n",
    "    N, dims = int(temp[0]), int(temp[1])\n",
    "    SE = np.zeros(shape=(N, dims), dtype=np.float32)\n",
    "    for line in lines[1:]:\n",
    "        temp = line.split(' ')\n",
    "        index = int(temp[0])\n",
    "        SE[index] = temp[1:]\n",
    "\n",
    "    # temporal embedding\n",
    "    Time = df.index\n",
    "    dayofweek = np.reshape(Time.weekday, newshape=(-1, 1))\n",
    "    timeofday = (Time.hour * 3600 + Time.minute * 60 + Time.second) \\\n",
    "        // Time.freq.delta.total_seconds()\n",
    "    timeofday = np.reshape(timeofday, newshape=(-1, 1))\n",
    "    Time = np.concatenate((dayofweek, timeofday), axis=-1)\n",
    "    # train/val/test\n",
    "    train = Time[: train_steps]\n",
    "    val = Time[train_steps: train_steps + val_steps]\n",
    "    test = Time[-test_steps:]\n",
    "    # shape = (num_sample, P + Q, 2)\n",
    "    trainTE = seq2instance(train, config.AGENT.HISTORY_STEPS,\n",
    "                           config.AGENT.PREDICTION_STEPS)\n",
    "    trainTE = np.concatenate(trainTE, axis=1).astype(np.int32)\n",
    "    valTE = seq2instance(val, config.AGENT.HISTORY_STEPS,\n",
    "                         config.AGENT.PREDICTION_STEPS)\n",
    "    valTE = np.concatenate(valTE, axis=1).astype(np.int32)\n",
    "    testTE = seq2instance(test, config.AGENT.HISTORY_STEPS,\n",
    "                          config.AGENT.PREDICTION_STEPS)\n",
    "    testTE = np.concatenate(testTE, axis=1).astype(np.int32)\n",
    "\n",
    "    return (trainX, trainTE, trainY, valX, valTE, valY, testX, testTE, testY,\n",
    "            SE, mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    dataset_file = os.path.join(config.PATH_TO_RECORDS, 'data', 'dataset.h5')\n",
    "\n",
    "    log_file = os.path.join(config.PATH_TO_RECORDS,\n",
    "                            f\"{config.EXPERIMENT.SCENARIO_NAME}_gman_log\")\n",
    "    log = open(os.path.join(config.ROOT_DIR, log_file), 'w')\n",
    "\n",
    "    model_file = os.path.join(config.PATH_TO_RECORDS,\n",
    "                              f\"{config.EXPERIMENT.SCENARIO_NAME}_gman_model\")\n",
    "\n",
    "    # load data\n",
    "    log_string(log, 'loading data...')\n",
    "    (trainX, trainTE, trainY, valX, valTE, valY, testX, testTE, testY, SE,\n",
    "     mean, std) = loadData(dataset_file, config.AGENT.PREDICTED_ATTRIBUTE)\n",
    "    log_string(log, 'trainX: %s\\ttrainY: %s' %\n",
    "                     (trainX.shape, trainY.shape))\n",
    "    log_string(log, 'valX:   %s\\t\\tvalY:   %s' %\n",
    "                     (valX.shape, valY.shape))\n",
    "    log_string(log, 'testX:  %s\\t\\ttestY:  %s' %\n",
    "                     (testX.shape, testY.shape))\n",
    "    log_string(log, 'data loaded!')\n",
    "\n",
    "    # train model\n",
    "    log_string(log, 'compiling ..')\n",
    "    T = 24 * 60 // config.AGENT.TIME_SLOT\n",
    "    num_train, _, N = trainX.shape\n",
    "    X, TE, label, is_training = placeholder(\n",
    "        config.AGENT.HISTORY_STEPS, config.AGENT.PREDICTION_STEPS, N)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    bn_momentum = tf.compat.v1.train.exponential_decay(\n",
    "        0.5, global_step,\n",
    "        decay_steps=config.AGENT.DECAY_EPOCH * num_train // config.AGENT.BATCH_SIZE,\n",
    "        decay_rate=0.5, staircase=True)\n",
    "    bn_decay = tf.minimum(0.99, 1 - bn_momentum)\n",
    "    pred = GMAN(\n",
    "        X,\n",
    "        TE,\n",
    "        SE,\n",
    "        T,\n",
    "        bn=True,\n",
    "        bn_decay=bn_decay,\n",
    "        is_training=is_training\n",
    "    )\n",
    "    pred = pred * std + mean\n",
    "    loss = mae_loss(pred, label)\n",
    "    tf.compat.v1.add_to_collection('pred', pred)\n",
    "    tf.compat.v1.add_to_collection('loss', loss)\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(\n",
    "        config.AGENT.LEARNING_RATE, global_step,\n",
    "        decay_steps=config.AGENT.DECAY_EPOCH * num_train // config.AGENT.BATCH_SIZE,\n",
    "        decay_rate=0.7, staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 1e-5)\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    parameters = 0\n",
    "    for variable in tf.compat.v1.trainable_variables():\n",
    "        parameters += np.product([x.value for x in variable.get_shape()])\n",
    "    log_string(log, 'trainable parameters: {:,}'.format(parameters))\n",
    "    log_string(log, 'model compiled!')\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    tf_config = tf.compat.v1.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config=tf_config)\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    log_string(log, '**** training model ****')\n",
    "    num_val = valX.shape[0]\n",
    "    wait = 0\n",
    "    val_loss_min = np.inf\n",
    "    for epoch in range(config.AGENT.MAX_EPOCH):\n",
    "        if wait >= config.AGENT.PATIENCE:\n",
    "            log_string(log, 'early stop at epoch: %04d' % (epoch))\n",
    "            break\n",
    "        # shuffle\n",
    "        permutation = np.random.permutation(num_train)\n",
    "        trainX = trainX[permutation]\n",
    "        trainTE = trainTE[permutation]\n",
    "        trainY = trainY[permutation]\n",
    "        # train loss\n",
    "        start_train = time.time()\n",
    "        train_loss = 0\n",
    "        num_batch = math.ceil(num_train / config.AGENT.BATCH_SIZE)\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * config.AGENT.BATCH_SIZE\n",
    "            end_idx = min(num_train, (batch_idx + 1) * config.AGENT.BATCH_SIZE)\n",
    "            feed_dict = {\n",
    "                X: trainX[start_idx:end_idx],\n",
    "                TE: trainTE[start_idx:end_idx],\n",
    "                label: trainY[start_idx:end_idx],\n",
    "                is_training: True}\n",
    "            _, loss_batch = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            train_loss += loss_batch * (end_idx - start_idx)\n",
    "        train_loss /= num_train\n",
    "        end_train = time.time()\n",
    "        # val loss\n",
    "        start_val = time.time()\n",
    "        val_loss = 0\n",
    "        num_batch = math.ceil(num_val / config.AGENT.BATCH_SIZE)\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * config.AGENT.BATCH_SIZE\n",
    "            end_idx = min(num_val, (batch_idx + 1) * config.AGENT.BATCH_SIZE)\n",
    "            feed_dict = {\n",
    "                X: valX[start_idx:end_idx],\n",
    "                TE: valTE[start_idx:end_idx],\n",
    "                label: valY[start_idx:end_idx],\n",
    "                is_training: False}\n",
    "            loss_batch = sess.run(loss, feed_dict=feed_dict)\n",
    "            val_loss += loss_batch * (end_idx - start_idx)\n",
    "        val_loss /= num_val\n",
    "        end_val = time.time()\n",
    "        log_string(\n",
    "            log,\n",
    "            '%s | epoch: %04d/%d, training time: %.1fs, inference time: %.1fs' %\n",
    "            (datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), epoch + 1,\n",
    "             config.AGENT.MAX_EPOCH, end_train - start_train, end_val - start_val))\n",
    "        log_string(\n",
    "            log, 'train loss: %.4f, val_loss: %.4f' % (train_loss, val_loss))\n",
    "        if val_loss <= val_loss_min:\n",
    "            log_string(\n",
    "                log,\n",
    "                'val loss decrease from %.4f to %.4f, saving model to %s' %\n",
    "                (val_loss_min, val_loss, os.path.join(config.ROOT_DIR, model_file)))\n",
    "            wait = 0\n",
    "            val_loss_min = val_loss\n",
    "            saver.save(sess, os.path.join(config.ROOT_DIR, model_file))\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "    # test model\n",
    "    log_string(log, '**** testing model ****')\n",
    "    log_string(log, 'loading model from %s' %\n",
    "                     os.path.join(config.ROOT_DIR, model_file))\n",
    "    saver = tf.compat.v1.train.import_meta_graph(\n",
    "        os.path.join(config.ROOT_DIR, model_file) + '.meta')\n",
    "    saver.restore(sess, os.path.join(config.ROOT_DIR, model_file))\n",
    "    log_string(log, 'model restored!')\n",
    "    log_string(log, 'evaluating...')\n",
    "    num_test = testX.shape[0]\n",
    "    trainPred = []\n",
    "    num_batch = math.ceil(num_train / config.AGENT.BATCH_SIZE)\n",
    "    for batch_idx in range(num_batch):\n",
    "        start_idx = batch_idx * config.AGENT.BATCH_SIZE\n",
    "        end_idx = min(num_train, (batch_idx + 1) * config.AGENT.BATCH_SIZE)\n",
    "        feed_dict = {\n",
    "            X: trainX[start_idx: end_idx],\n",
    "            TE: trainTE[start_idx: end_idx],\n",
    "            is_training: False}\n",
    "        pred_batch = sess.run(pred, feed_dict=feed_dict)\n",
    "        trainPred.append(pred_batch)\n",
    "    trainPred = np.concatenate(trainPred, axis=0)\n",
    "    valPred = []\n",
    "    num_batch = math.ceil(num_val / config.AGENT.BATCH_SIZE)\n",
    "    for batch_idx in range(num_batch):\n",
    "        start_idx = batch_idx * config.AGENT.BATCH_SIZE\n",
    "        end_idx = min(num_val, (batch_idx + 1) * config.AGENT.BATCH_SIZE)\n",
    "        feed_dict = {\n",
    "            X: valX[start_idx: end_idx],\n",
    "            TE: valTE[start_idx: end_idx],\n",
    "            is_training: False}\n",
    "        pred_batch = sess.run(pred, feed_dict=feed_dict)\n",
    "        valPred.append(pred_batch)\n",
    "    valPred = np.concatenate(valPred, axis=0)\n",
    "    testPred = []\n",
    "    num_batch = math.ceil(num_test / config.AGENT.BATCH_SIZE)\n",
    "    start_test = time.time()\n",
    "    for batch_idx in range(num_batch):\n",
    "        start_idx = batch_idx * config.AGENT.BATCH_SIZE\n",
    "        end_idx = min(num_test, (batch_idx + 1) * config.AGENT.BATCH_SIZE)\n",
    "        feed_dict = {\n",
    "            X: testX[start_idx:end_idx],\n",
    "            TE: testTE[start_idx:end_idx],\n",
    "            is_training: False}\n",
    "        pred_batch = sess.run(pred, feed_dict=feed_dict)\n",
    "        testPred.append(pred_batch)\n",
    "    end_test = time.time()\n",
    "    testPred = np.concatenate(testPred, axis=0)\n",
    "    train_mae, train_rmse, train_mape = metric(trainPred, trainY)\n",
    "    val_mae, val_rmse, val_mape = metric(valPred, valY)\n",
    "    test_mae, test_rmse, test_mape = metric(testPred, testY)\n",
    "    log_string(log, 'testing time: %.1fs' % (end_test - start_test))\n",
    "    log_string(log, '                MAE\\t\\tRMSE\\t\\tMAPE')\n",
    "    log_string(log, 'train            %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "                     (train_mae, train_rmse, train_mape * 100))\n",
    "    log_string(log, 'val              %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "                     (val_mae, val_rmse, val_mape * 100))\n",
    "    log_string(log, 'test             %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "                     (test_mae, test_rmse, test_mape * 100))\n",
    "    log_string(log, 'performance in each prediction step')\n",
    "    MAE, RMSE, MAPE = [], [], []\n",
    "    for q in range(config.AGENT.PREDICTION_STEPS):\n",
    "        mae, rmse, mape = metric(testPred[:, q], testY[:, q])\n",
    "        MAE.append(mae)\n",
    "        RMSE.append(rmse)\n",
    "        MAPE.append(mape)\n",
    "        log_string(log, 'step: %02d         %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "                         (q + 1, mae, rmse, mape * 100))\n",
    "    average_mae = np.mean(MAE)\n",
    "    average_rmse = np.mean(RMSE)\n",
    "    average_mape = np.mean(MAPE)\n",
    "    log_string(\n",
    "        log, 'average:         %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "             (average_mae, average_rmse, average_mape * 100))\n",
    "    end = time.time()\n",
    "    log_string(log, 'total time: %.1fmin' % ((end - start) / 60))\n",
    "    sess.close()\n",
    "    log.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
